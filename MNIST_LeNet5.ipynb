{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST - LeNet5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OkM1sruoAjh"
      },
      "source": [
        "# Reconhecimento de dígitos utilizando a arquitetura LeNet-5 com o conjunto de dados MNIST\n",
        "\n",
        "Esse projeto possui o objetivo de utilizar a biblioteca Keras para desenvolver um modelo de rede neural convolucional para identificar padrões na base de dados MNIST, que consiste em 60000 imagens de dígitos escritos à mão. \n",
        "\n",
        "Primeiramente serão feitas uma análise do dataset, uma exploração do conjunto de dados e a preparação do modelo, para posterior validação e teste.\n",
        "\n",
        "O modelo convolucional estudado se trata da arquitetura clássica LeNet-5. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LHi1xoeoOAW"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Primeiramente, precisamos acessar o dataset que será utilizado para treinar e testar a nossa rede neural convolucional. O conjunto de dados foi retirado do site oficial do criador dessa arquitetura, [Yann LeCun](http://yann.lecun.com/exdb/mnist/). O dataset consiste em:\n",
        "\n",
        "* train-images-idx3-ubyte.gz:  imagens de treino\n",
        "* train-labels-idx1-ubyte.gz:  labels para as imagens de treino \n",
        "* t10k-images-idx3-ubyte.gz:   imagens de teste\n",
        "* t10k-labels-idx1-ubyte.gz:   labels para as imagens de teste\n",
        "\n",
        "Vamos criar uma função para baixar o conjunto de dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy-UUPJUlocs"
      },
      "source": [
        "from requests import get\n",
        "\n",
        "def download_file(url, file_name):\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        response = get(url)\n",
        "        file.write(response.content)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQzpfqTklqeI"
      },
      "source": [
        "download_file('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', 'train-images-idx3-ubyte.gz')\n",
        "download_file('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', 'train-labels-idx1-ubyte.gz')\n",
        "download_file('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz')\n",
        "download_file('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', 't10k-labels-idx1-ubyte.gz')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX-QisZNuQFv"
      },
      "source": [
        "Agora vamos carregar os dados de teste e treino na memória."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue6BfuJql24p"
      },
      "source": [
        "#Bibliotecas auxiliares para leitura \n",
        "import gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRxyU6RMl8Ak"
      },
      "source": [
        "def read_gzip(images_path: str, labels_path: str):\n",
        "    #lendo as labels\n",
        "    with gzip.open(labels_path, 'rb') as labels:\n",
        "        labels = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
        "\n",
        "    #lendo as features\n",
        "    with gzip.open(images_path,'rb') as images:\n",
        "        length = len(labels)\n",
        "        #Primeiramente carregamos as imagens que estão \"esticadas\" em um vetor \n",
        "        #de 784 números (imagens com 28x28 pixels) e as convertemos em 28x28x1\n",
        "        features = np.frombuffer(images.read(), dtype=np.uint8, offset=16) \\\n",
        "                        .reshape(length, 784) \\\n",
        "                        .reshape(length, 28, 28, 1)\n",
        "        \n",
        "    return features, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GUiIZegl-Ta"
      },
      "source": [
        "#train e test serão dicionários que guardarão as features e os labels das imagens\n",
        "train = {}\n",
        "test = {}\n",
        "\n",
        "train['features'], train['labels'] = read_gzip('train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz')\n",
        "test['features'], test['labels'] = read_gzip('t10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aV4FqgmyAcp"
      },
      "source": [
        "Vamos agora verificar o número de imagens nos training e test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvw7qSc8mB9u",
        "outputId": "d2014612-0dcb-4e16-c8af-71b19bfd6984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('# de imagens de treino:', train['features'].shape[0])\n",
        "print('# de imagens de teste:', test['features'].shape[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# de imagens de treino: 60000\n",
            "# de imagens de teste: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbG0C4p4zh5s"
      },
      "source": [
        "## Explorando o conjunto de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFVKiKYizFsu"
      },
      "source": [
        "#### Agora vamos criar uma função para olhar algumas imagens do dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C85WWBeAmDX_"
      },
      "source": [
        "def display_image(position):\n",
        "    image = train['features'][position].squeeze()\n",
        "    plt.title('Example %d. Label: %d' % (position, train['labels'][position]))\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmK3iRGkmFYi",
        "outputId": "e60dc1d0-5678-4bc8-e30c-1ca76ff79e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "display_image(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Bh1PRtmHk1",
        "outputId": "8b5c04b0-be2e-4eff-d65c-c80f40f0b1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "display_image(4)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4-7AkB9z29n"
      },
      "source": [
        "#### Verificando se o conjunto de dados está balanceado, ou seja, se existem quantidades parecidas de dados para cada label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjzypz4hmK-6",
        "outputId": "bf1a41c1-19a6-49c5-cf73-996b253aebde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "train_labels_count = np.unique(train['labels'], return_counts=True)\n",
        "dataframe_train_labels = pd.DataFrame({'Label':train_labels_count[0], 'Count':train_labels_count[1]})\n",
        "dataframe_train_labels"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>5918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>6265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>5851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>5949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label  Count\n",
              "0      0   5923\n",
              "1      1   6742\n",
              "2      2   5958\n",
              "3      3   6131\n",
              "4      4   5842\n",
              "5      5   5421\n",
              "6      6   5918\n",
              "7      7   6265\n",
              "8      8   5851\n",
              "9      9   5949"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1w5rg6s0EMZ"
      },
      "source": [
        "#### Vamos agora dividir o conjunto de dados de treinamento em dados para treinamento e dados para validação. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knRfEqIO0LzW"
      },
      "source": [
        "O Training set é o conjunto de dados que é utilizado de fato para treinamento, e serve para encontrarmos pesos (idealmente ótimos) utilizando a técnica de back-propagation.\n",
        "\n",
        "Já o Validation set é utilizado para tunar os parâmetros da rede, como número de hidden units, ou determinar um ponto de parada para o algoritmo de back-propagation. Ao \"tunarmos\" os parâmetros da rede utilizando o Validation set, adicionamos um viés a esse conjunto de dados, logo ele não serve para testar a real performance do nosso modelo, assim precisamos do Test set. \n",
        "\n",
        "O ideal é não modificar a rede utilizando como base o Test set, pois como explicado anteriormente, adicionaremos um viés ao conjunto de dados e as métricas de desempenho não representarão o real desempenho do modelo. \n",
        "\n",
        "Dividiremos o Training set anterior então em 80% dos dados para o novo Training set e 20% dos dados para o Validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDDS081pmQI5"
      },
      "source": [
        "#Separando o Training set em Training e Validation set\n",
        "validation = {}\n",
        "train['features'], validation['features'], train['labels'], validation['labels'] = train_test_split(train['features'], train['labels'], test_size=0.2, random_state=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFUuVraxmTsE",
        "outputId": "db831cfb-4719-4ad3-e079-8a172881da1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('# de imagens de treinamento:', train['features'].shape[0])\n",
        "print('# de imagens de validação:', validation['features'].shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# de imagens de treinamento: 48000\n",
            "# de imagens de validação: 12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIUEzfxD3AqP"
      },
      "source": [
        "## Preparando as features\n",
        "\n",
        "A arquitetura LeNet-5 utiliza como entrada imagens de 32x32 pixels. Como temos imagens com 28x28 pixels, precisamos fazer a técnica *zero padding*, adicionando 2 pixels em cada lado da imagem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3MUDwRgmU_c",
        "outputId": "095d666e-88fc-4321-ad41-5c3e61b49e0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Realizando a técnica de zero padding no conjunto de dados\n",
        "train['features']      = np.pad(train['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "validation['features'] = np.pad(validation['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "test['features']       = np.pad(test['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "    \n",
        "print(\"Updated Image Shape: {}\".format(train['features'][0].shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated Image Shape: (32, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L70VqYVH30F_"
      },
      "source": [
        "## Implementação da arquitetura LeNet-5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B8oR8vhFsVR"
      },
      "source": [
        "A arquitetura a ser implementada consiste nos seguintes layers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unRtScMYFHfr"
      },
      "source": [
        "![lenet.png](https://raw.githubusercontent.com/MostafaGazar/mobile-ml/master/files/lenet.png)\n",
        ">>> LeNet-5 Architecture. Credit: [LeCun et al., 1998](http://yann.lecun.com/exdb/publis/psgz/lecun-98.ps.gz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSRtZbVeFnSB"
      },
      "source": [
        "#### Architecture\n",
        "* **Convolutional Layer #1** resulta em uma saída com dimensões 28x28x6.\n",
        "    * **Ativação** utilizaremos uma relu\n",
        "\n",
        "* **Pooling Layer #1** resulta em uma saída com dimensões 14x14x6.\n",
        "\n",
        "* **Convolutional Layer #2** resulta em uma saída com dimensões 10x10x16.\n",
        "    * **Ativação** utilizaremos uma relu\n",
        "\n",
        "* **Pooling Layer #2** resulta em uma saída com dimensões 5x5x16.\n",
        "    * **Flatten** \"planariza\" a dimensão da saída\n",
        "\n",
        "* **Fully Connected Layer #1** resulta em uma saída com dimensões 120x1.\n",
        "    * **Ativação** utilizaremos uma relu\n",
        "\n",
        "* **Fully Connected Layer #2** resulta em uma saída com dimensões 84x1.\n",
        "    * **Ativação** utilizaremos uma relu\n",
        "\n",
        "* **Fully Connected (Logits) #3** resulta em uma saída de dimensão 10, sendo as probabilidades das imagens serem números de 0 a 9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm2XbRGimXlH"
      },
      "source": [
        "#Definindo a arquitetura do modelo\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\n",
        "model.add(layers.AveragePooling2D())\n",
        "\n",
        "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.AveragePooling2D())\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=120, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(units=84, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(units=10, activation = 'softmax'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYitgsUPmZx5",
        "outputId": "97fc6e4f-93fb-45d6-f823-6fa9e55aebe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 6)         60        \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 15, 15, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 16)        880       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 120)               69240     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 81,194\n",
            "Trainable params: 81,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHjwtVbzmbUj"
      },
      "source": [
        "#Definindo qual será a função de loss, o otimizador e a métrica a ser otimizada\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9T5G2x_mhDZ"
      },
      "source": [
        "#Definindo o número de épocas que o modelo vai executar e o tamanho do lote \n",
        "#dos dados\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffDRjuSgmdQJ"
      },
      "source": [
        "X_train, y_train = train['features'], to_categorical(train['labels'])\n",
        "X_validation, y_validation = validation['features'], to_categorical(validation['labels'])\n",
        "X_test, y_test = test['features'], to_categorical(test['labels'])\n",
        "\n",
        "train_generator = ImageDataGenerator().flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "validation_generator = ImageDataGenerator().flow(X_validation, y_validation, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCJEv1DFmfL-",
        "outputId": "014ba54e-3b48-4350-f2b0-5ba2a3c7ef21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "print('# de imagens de treinamento:', train['features'].shape[0])\n",
        "print('# de imagens de validação:', validation['features'].shape[0])\n",
        "\n",
        "steps_per_epoch = X_train.shape[0]//BATCH_SIZE\n",
        "validation_steps = X_validation.shape[0]//BATCH_SIZE\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, \n",
        "                    validation_data=validation_generator, validation_steps=validation_steps, \n",
        "                    shuffle=True, callbacks=[tensorboard])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# de imagens de treinamento: 48000\n",
            "# de imagens de validação: 12000\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0585 - val_accuracy: 0.9856\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 24s 63ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.0565 - val_accuracy: 0.9868\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0554 - val_accuracy: 0.9881\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0555 - val_accuracy: 0.9878\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0522 - val_accuracy: 0.9862\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0534 - val_accuracy: 0.9853\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0543 - val_accuracy: 0.9879\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0643 - val_accuracy: 0.9860\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.0596 - val_accuracy: 0.9873\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 23s 61ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0703 - val_accuracy: 0.9863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6b4240aba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdxDCV39dJWP"
      },
      "source": [
        "Como pode ser conferido, a rede obteve uma acurácia de 99.82% na validação após 10 épocas de treinamento. É um resultado bem expressivo. Agora vamos avaliar o modelo no Test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mtwocHjd4p_",
        "outputId": "255798fd-0ad8-4381-d451-e437422176c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "_, accuracy = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0682 - accuracy: 0.9860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_5ObUNafy_8"
      },
      "source": [
        "No Test set a nossa acurácia foi de 98.6%, também representando um ótimo desempenho. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-m1Ij2wi2iV",
        "outputId": "e6d5cd6e-2f9f-43b9-a705-0009d5d8fdbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Salvando as saídas da rede para o Test set\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "#Transformando o array em um array unidimensional\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "Y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "y_actu = pd.Series(Y_true, name='Actual')\n",
        "y_pred = pd.Series(Y_pred_classes, name='Predicted')\n",
        "df_confusion = pd.crosstab(y_actu, y_pred)\n",
        "\n",
        "print(df_confusion)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted    0     1     2     3    4    5    6     7    8     9\n",
            "Actual                                                          \n",
            "0          966     0     1     0    2    0    4     3    0     4\n",
            "1            0  1129     2     3    0    0    0     1    0     0\n",
            "2            0     0  1017     2    0    0    0    12    0     1\n",
            "3            0     0     1  1003    0    1    0     4    1     0\n",
            "4            0     0     0     0  971    0    0     0    0    11\n",
            "5            2     0     0    14    0  863    4     1    1     7\n",
            "6            3     2     0     0    1    1  950     0    1     0\n",
            "7            0     1     2     0    0    0    0  1023    0     2\n",
            "8            1     1     5     6    1    0    3     8  938    11\n",
            "9            0     1     0     1    4    0    0     2    1  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nras8qhwl1-P"
      },
      "source": [
        "Como pode ser conferido pela Confusion Matrix acima, os erros comuns cometidos pela rede são:\n",
        "\n",
        "* Classificar 3 quando na realidade o número é 5 (ocorrência: 14)\n",
        "* Classificar 7 quando na realidade o número é 2 (ocorrência: 12)\n",
        "* Classificar 9 quando na realidade o número é 8 (ocorrência: 11)\n",
        "* Classificar 9 quando na realidade o número é 4 (ocorrência: 11)\n",
        "\n",
        "Além desses erros não foram identificandas fontes de erro muito relevantes, e no geral o modelo desempenhou um bom papel na classificação dos dígitos."
      ]
    }
  ]
}